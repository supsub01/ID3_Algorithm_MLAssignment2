{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision_tree.py\n",
    "# ---------\n",
    "# Licensing Information:  You are free to use or extend these projects for\n",
    "# personal and educational purposes provided that (1) you do not distribute\n",
    "# or publish solutions, (2) you retain this notice, and (3) you provide clear\n",
    "# attribution to UT Dallas, including a link to http://cs.utdallas.edu.\n",
    "#\n",
    "# This file is part of Homework for CS6375: Machine Learning.\n",
    "# Gautam Kunapuli (gautam.kunapuli@utdallas.edu)\n",
    "# Sriraam Natarajan (sriraam.natarajan@utdallas.edu),\n",
    "# Anjum Chida (anjum.chida@utdallas.edu)\n",
    "#\n",
    "#\n",
    "# INSTRUCTIONS:\n",
    "# ------------\n",
    "# 1. This file contains a skeleton for implementing the ID3 algorithm for\n",
    "# Decision Trees. Insert your code into the various functions that have the\n",
    "# comment \"INSERT YOUR CODE HERE\".\n",
    "#\n",
    "# 2. Do NOT modify the classes or functions that have the comment \"DO NOT\n",
    "# MODIFY THIS FUNCTION\".\n",
    "#\n",
    "# 3. Do not modify the function headers for ANY of the functions.\n",
    "#\n",
    "# 4. You may add any other helper functions you feel you may need to print,\n",
    "# visualize, test, or save the data and results. However, you MAY NOT utilize\n",
    "# the package scikit-learn OR ANY OTHER machine learning package in THIS file.\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "#import graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {1: [0], 2: [1], 3: [2, 3]})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def partition(x):\n",
    "    \"\"\"\n",
    "    Partition the column vector x into subsets indexed by its unique values (v1, ... vk)\n",
    "\n",
    "    Returns a dictionary of the form\n",
    "    { v1: indices of x == v1,\n",
    "      v2: indices of x == v2,\n",
    "      ...\n",
    "      vk: indices of x == vk }, where [v1, ... vk] are all the unique values in the vector z.\n",
    "    \"\"\"\n",
    "\n",
    "    # INSERT YOUR CODE HERE\n",
    "    Dict = defaultdict(list)\n",
    "    index=0\n",
    "    for i in x:\n",
    "        if i[0] in Dict:\n",
    "            Dict[i[0]].append(index)\n",
    "        else:\n",
    "            Dict.setdefault(i[0],[]).append(index)\n",
    "        index+=1\n",
    "    \n",
    "    return Dict        \n",
    "    raise Exception('Function not yet implemented!')\n",
    "    \n",
    "# #DEBUG\n",
    "# x = np.array([[1],[2],[3],[3]]) \n",
    "# partition(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    \"\"\"\n",
    "    Compute the entropy of a vector y by considering the counts of the unique values (v1, ... vk), in z\n",
    "\n",
    "    Returns the entropy of z: H(z) = p(z=v1) log2(p(z=v1)) + ... + p(z=vk) log2(p(z=vk))\n",
    "    \"\"\"\n",
    "\n",
    "    # INSERT YOUR CODE HERE\n",
    "    n = len(y)\n",
    "    \n",
    "    unique_freq = np.bincount(y)\n",
    "    p = unique_freq[np.nonzero(unique_freq)] / n\n",
    "    \n",
    "    return - np.sum(p * np.log2(p))\n",
    "\n",
    "    raise Exception('Function not yet implemented!')\n",
    "\n",
    "#DEBUG\n",
    "#x = np.array([1,2,1,2]) \n",
    "#entropy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.45914792, 0.25162917, 0.10917034])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def conditional_entropy(x, y):\n",
    "    xy = np.c_[x, y]\n",
    "    \n",
    "    unique_freq = np.nonzero(np.bincount(x))\n",
    "    #p_v is the probability of x=v. Input x_v is all ys where x=v \n",
    "    unique_freq = np.bincount(x)\n",
    "    p_v = unique_freq[np.nonzero(unique_freq)] / len(x)\n",
    "    p_v_not = 1-p_v\n",
    "    entropies=[]\n",
    "    entropies_rest=[]\n",
    "    #H_yx=np.sum(p_v*entropy(yx_v))\n",
    "    for v in set(x):\n",
    "        y_new=[]\n",
    "        y_rest=[]\n",
    "        for element in xy:\n",
    "            if element[0]==v:\n",
    "                y_new.append(element[1])\n",
    "            else:\n",
    "                y_rest.append(element[1])\n",
    "        entropies_rest.append(entropy(y_rest))        \n",
    "        entropies.append(entropy(y_new))\n",
    "    weighted_entropies=p_v*entropies+(p_v_not*entropies_rest)\n",
    "    #print(weighted_entropies)\n",
    "    return np.asarray(weighted_entropies)\n",
    "    #return np.sum(p_v*entropies)\n",
    "\n",
    "def mutual_information(x, y):\n",
    "    \"\"\"\n",
    "    Compute the mutual information between a data column (x) and the labels (y). The data column is a single attribute\n",
    "    over all the examples (n x 1). Mutual information is the difference between the entropy BEFORE the split set, and\n",
    "    the weighted-average entropy of EACH possible split.\n",
    "\n",
    "    Returns the mutual information: I(x, y) = H(y) - H(y | x)\n",
    "    \"\"\"\n",
    "\n",
    "    # INSERT YOUR CODE HERE\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    H_y = entropy(y)\n",
    "    H_y_vector=np.full((len(set(x)),1), H_y)\n",
    "    #print(H_y_vector)\n",
    "    \n",
    "    H_yx=conditional_entropy(x,y)\n",
    "\n",
    "    # Mutual Information\n",
    "    I_xy =H_y - H_yx\n",
    "    \n",
    "    return  I_xy\n",
    "\n",
    "    raise Exception('Function not yet implemented!')\n",
    "    \n",
    "#DEBUG\n",
    "x = np.array([2,3,5,3,2,2])\n",
    "y = np.array([1,1,1,1,0,0])\n",
    "mutual_information(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_att_val_pairs(x):\n",
    "    attribute_value_pairs=[]\n",
    "    col_num=0\n",
    "    \n",
    "    for column in x.T:\n",
    "        for x in set(column):\n",
    "            attribute_value_pairs.append(tuple((col_num,x)))\n",
    "        col_num+=1\n",
    "    return attribute_value_pairs\n",
    "##DEBUG\n",
    "# x = np.array([[1,0,5],[2,0,6],[3,1,5],[1,1,6],[2,0,6],[3,0,6]])\n",
    "# create_att_val_pairs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id3_love(x, y, attribute_value_pairs=None, depth=0, max_depth=5):\n",
    "    \"\"\"\n",
    "    Implements the classical ID3 algorithm given training data (x), training labels (y) and an array of\n",
    "    attribute-value pairs to consider. This is a recursive algorithm that depends on three termination conditions\n",
    "        1. If the entire set of labels (y) is pure (all y = only 0 or only 1), then return that label\n",
    "        2. If the set of attribute-value pairs is empty (there is nothing to split on), then return the most common\n",
    "           value of y (majority label)\n",
    "        3. If the max_depth is reached (pre-pruning bias), then return the most common value of y (majority label)\n",
    "    Otherwise the algorithm selects the next best attribute-value pair using INFORMATION GAIN as the splitting criterion\n",
    "    and partitions the data set based on the values of that attribute before the next recursive call to ID3.\n",
    "\n",
    "    The tree we learn is a BINARY tree, which means that every node has only two branches. The splitting criterion has\n",
    "    to be chosen from among all possible attribute-value pairs. That is, for a problem with two features/attributes x1\n",
    "    (taking values a, b, c) and x2 (taking values d, e), the initial attribute value pair list is a list of all pairs of\n",
    "    attributes with their corresponding values:\n",
    "    [(x1, a),\n",
    "     (x1, b),\n",
    "     (x1, c),\n",
    "     (x2, d),\n",
    "     (x2, e)]\n",
    "     If we select (x2, d) as the best attribute-value pair, then the new decision node becomes: [ (x2 == d)? ] and\n",
    "     the attribute-value pair (x2, d) is removed from the list of attribute_value_pairs.\n",
    "\n",
    "    The tree is stored as a nested dictionary, where each entry is of the form\n",
    "                    (attribute_index, attribute_value, True/False): subtree\n",
    "    * The (attribute_index, attribute_value) determines the splitting criterion of the current node. For example, (4, 2)\n",
    "    indicates that we test if (x4 == 2) at the current node.\n",
    "    * The subtree itself can be nested dictionary, or a single label (leaf node).\n",
    "    * Leaf nodes are (majority) class labels\n",
    "\n",
    "    Returns a decision tree represented as a nested dictionary, for example\n",
    "    {(4, 1, False):\n",
    "        {(0, 1, False):\n",
    "            {(1, 1, False): 1,\n",
    "             (1, 1, True): 0},\n",
    "         (0, 1, True):\n",
    "            {(1, 1, False): 0,\n",
    "             (1, 1, True): 1}},\n",
    "     (4, 1, True): 1}\n",
    "    \"\"\"\n",
    "    if attribute_value_pairs == None :\n",
    "        attribute_value_pairs = []\n",
    "        Shape_x=x.shape[1]\n",
    "        for i in range(0, Shape_x):\n",
    "            for j in set(x[:, i]):\n",
    "                attribute_value_pairs.append((i,j))\n",
    "\n",
    "    if len(attribute_value_pairs) == 0 or depth == max_depth :\n",
    "        count = np.bincount(np.array(y))\n",
    "        val= np.argmax(count)\n",
    "        return val\n",
    "    \n",
    "    elif all(z == y[0] for z in y) :\n",
    "        return y[0]\n",
    "    \n",
    "    else :\n",
    "        val_max = 0\n",
    "        for item in attribute_value_pairs:\n",
    "            tmp = []\n",
    "            k = item[0]\n",
    "            length=len(x)\n",
    "            for i in range(0, length):\n",
    "                w = x[i][k]\n",
    "                if w == item[1]:\n",
    "                    tmp.append(1)\n",
    "                else:\n",
    "                    tmp.append(0)\n",
    "            val = mutual_information(tmp,y)\n",
    "            if val >= val_max:\n",
    "                val_max = val\n",
    "                split_find = item\n",
    "        \n",
    "        w = split_find[1]\n",
    "        k = split_find[0]\n",
    "        tmp = []\n",
    "        for i in range(0, len(x)):\n",
    "            tmp.append(x[i][k])\n",
    "            \n",
    "        final =partition(tmp)[w]\n",
    "        \n",
    "        T_X = []\n",
    "        F_X = []\n",
    "        T_Y = []\n",
    "        F_Y = []\n",
    "        \n",
    "        length=len(x)\n",
    "        for i in range(0, length):\n",
    "            temp = np.asarray(x[i])\n",
    "            if i in final:\n",
    "                T_X.append(temp)\n",
    "                T_Y.append(y[i])\n",
    "            else:\n",
    "                F_X.append(temp)\n",
    "                F_Y.append(y[i])\n",
    "        \n",
    "        T = attribute_value_pairs.copy()\n",
    "        F = attribute_value_pairs.copy()\n",
    "        T.remove(split_find)\n",
    "        F.remove(split_find)\n",
    "        \n",
    "        final_tree=dict()\n",
    "        final_tree.update({(split_find[0], split_find[1], True): id3_love(T_X, T_Y, T, depth+1, max_depth)})\n",
    "        final_tree.update({(split_find[0], split_find[1], False): id3_love(F_X, F_Y, F, depth+1, max_depth)})\n",
    "        return final_tree\n",
    "\n",
    "#DEBUG\n",
    "x1 = [0, 1, 1, 2, 2, 2]\n",
    "x2 = [0, 0, 1, 1, 1, 0]\n",
    "y = np.array([0, 0, 0, 1, 1, 0]) \n",
    "X = np.array([x1, x2]).T\n",
    "print(id3_love(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id3(x, y, attribute_value_pairs=None, depth=0, max_depth=5):\n",
    "    \"\"\"\n",
    "    Implements the classical ID3 algorithm given training data (x), training labels (y) and an array of\n",
    "    attribute-value pairs to consider. This is a recursive algorithm that depends on three termination conditions\n",
    "        1. If the entire set of labels (y) is pure (all y = only 0 or only 1), then return that label\n",
    "        2. If the set of attribute-value pairs is empty (there is nothing to split on), then return the most common\n",
    "           value of y (majority label)\n",
    "        3. If the max_depth is reached (pre-pruning bias), then return the most common value of y (majority label)\n",
    "    Otherwise the algorithm selects the next best attribute-value pair using INFORMATION GAIN as the splitting criterion\n",
    "    and partitions the data set based on the values of that attribute before the next recursive call to ID3.\n",
    "\n",
    "    The tree we learn is a BINARY tree, which means that every node has only two branches. The splitting criterion has\n",
    "    to be chosen from among all possible attribute-value pairs. That is, for a problem with two features/attributes x1\n",
    "    (taking values a, b, c) and x2 (taking values d, e), the initial attribute value pair list is a list of all pairs of\n",
    "    attributes with their corresponding values:\n",
    "    [(x1, a),\n",
    "     (x1, b),\n",
    "     (x1, c),\n",
    "     (x2, d),\n",
    "     (x2, e)]\n",
    "     If we select (x2, d) as the best attribute-value pair, then the new decision node becomes: [ (x2 == d)? ] and\n",
    "     the attribute-value pair (x2, d) is removed from the list of attribute_value_pairs.\n",
    "\n",
    "    The tree is stored as a nested dictionary, where each entry is of the form\n",
    "                    (attribute_index, attribute_value, True/False): subtree\n",
    "    * The (attribute_index, attribute_value) determines the splitting criterion of the current node. For example, (4, 2)\n",
    "    indicates that we test if (x4 == 2) at the current node.\n",
    "    * The subtree itself can be nested dictionary, or a single label (leaf node).\n",
    "    * Leaf nodes are (majority) class labels\n",
    "\n",
    "    Returns a decision tree represented as a nested dictionary, for example\n",
    "    {(4, 1, False):\n",
    "        {(0, 1, False):\n",
    "            {(1, 1, False): 1,\n",
    "             (1, 1, True): 0},\n",
    "         (0, 1, True):\n",
    "            {(1, 1, False): 0,\n",
    "             (1, 1, True): 1}},\n",
    "     (4, 1, True): 1}\n",
    "    \"\"\"\n",
    "\n",
    "    # INSERT YOUR CODE HERE. NOTE: THIS IS A RECURSIVE FUNCTION.\n",
    "    if (y.size==0) :\n",
    "        return 0\n",
    "    #If all set of labels(y) is pure\n",
    "    print(\"This is y\")\n",
    "    print(y)\n",
    "    print(np.argmax(np.bincount(y)))\n",
    "    \n",
    "    if np.unique(y).size==1 :\n",
    "        print(\"1\")\n",
    "        return np.argmax(np.bincount(y))\n",
    "    #If the set of attribute-value pairs is empty\n",
    "    if depth>0 and len(attribute_value_pairs)==0:\n",
    "        print(\"2\")\n",
    "        return np.argmax(np.bincount(y))\n",
    "    # If the max_depth is reached\n",
    "    if max_depth==depth:\n",
    "        print(\"3\")\n",
    "        return np.argmax(np.bincount(y))\n",
    "         #Do we need to change the tree to associate the leaf nodes to the majority class for the last two anchors?\n",
    "    \n",
    "    #Creating attribute_value_pairs list\n",
    "    attribute_value_pairs=create_att_val_pairs(x)\n",
    "    \n",
    "    #Calculate the Mutual Information for each of the features\n",
    "    column_num=0\n",
    "    selected_attribute_for_split_index=0;\n",
    "    mutual_info=np.array([])\n",
    "    for column in x.T:\n",
    "        res=mutual_information(column,y)\n",
    "        for element in res:\n",
    "            mutual_info=np.append(mutual_info,np.array([element]), axis=0)\n",
    "        if(mutual_info.size==0):\n",
    "            return 0\n",
    "    selected_attribute_for_split_index = np.argmin(mutual_info)\n",
    "    \n",
    "    \n",
    "    #Remove from the attribute list \n",
    "    dummy=attribute_value_pairs[selected_attribute_for_split_index]\n",
    "    del attribute_value_pairs[selected_attribute_for_split_index]\n",
    "\n",
    "    #*****THIS IS THE PART THAT NEEDS TO BE EDITED*****\n",
    "    # Add to the final tree\n",
    "    # We split using the selected attribute\n",
    "    sets = partition(np.asarray(x[:, selected_attribute_for_split_index]).reshape(len(x),1))\n",
    "    \n",
    "    #Recursive call  --make this binary\n",
    "    res = {}\n",
    "    depth+=1\n",
    "    y_subset=np.array([])\n",
    "    x_subset=np.array([])\n",
    "    for k, v in sets.items():\n",
    "        if k==dummy[1]:\n",
    "            y_1 = y.take(v, axis=0)\n",
    "            x_1 = x.take(v, axis=0)\n",
    "            break\n",
    "    \n",
    "    y_subset=np.delete(y,v)\n",
    "    x_subset=np.delete(x,v,0)\n",
    "    \n",
    "#     res['(%d, %d, %s)' % (selected_attribute_for_split_index+1, k, \"True\")] = id3(np.asarray(x_1), np.asarray(y_1),attribute_value_pairs, depth, max_depth=5)\n",
    "#     res['(%d, %d, %s)' % (selected_attribute_for_split_index+1, dummy[1], \"False\")] = id3(np.asarray(x_subset), np.asarray(y_subset),attribute_value_pairs, depth, max_depth=5)\n",
    "        \n",
    "#     return res\n",
    "    final_tree=dict()\n",
    "    final_tree.update({(selected_attribute_for_split_index+1, k, True): id3(np.asarray(x_1), np.asarray(y_1), attribute_value_pairs, depth, max_depth)})\n",
    "    final_tree.update({(selected_attribute_for_split_index+1, dummy[1], False): id3(np.asarray(x_subset), np.asarray(y_subset),attribute_value_pairs, depth, max_depth)})\n",
    "    return final_tree\n",
    "    raise Exception('Function not yet implemented!')\n",
    "    \n",
    "#DEBUG\n",
    "x1 = [0, 1, 1, 2, 2, 2]\n",
    "x2 = [0, 0, 1, 1, 1, 0]\n",
    "y = np.array([0, 0, 0, 1, 1, 0]) \n",
    "X = np.array([x1, x2]).T\n",
    "\n",
    "id3(X, y)\n",
    "#pretty_print(id3(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_example(x, tree):\n",
    "    \"\"\"\n",
    "    Predicts the classification label for a single example x using tree by recursively descending the tree until\n",
    "    a label/leaf node is reached.\n",
    "\n",
    "    Returns the predicted label of x according to tree\n",
    "    \"\"\"\n",
    "\n",
    "    # INSERT YOUR CODE HERE. NOTE: THIS IS A RECURSIVE FUNCTION.\n",
    "    raise Exception('Function not yet implemented!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the average error between the true labels (y_true) and the predicted labels (y_pred)\n",
    "\n",
    "    Returns the error = (1/n) * sum(y_true != y_pred)\n",
    "    \"\"\"\n",
    "\n",
    "    # INSERT YOUR CODE HERE\n",
    "    return np.sum(np.absolute(np.asarray(y_true)-np.asarray(y_pred)))/len(y_true)\n",
    "\n",
    "    raise Exception('Function not yet implemented!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(tree, depth=0):\n",
    "    \"\"\"\n",
    "    Pretty prints the decision tree to the console. Use print(tree) to print the raw nested dictionary representation\n",
    "    DO NOT MODIFY THIS FUNCTION!\n",
    "    \"\"\"\n",
    "    if depth == 0:\n",
    "        print('TREE')\n",
    "\n",
    "    for index, split_criterion in enumerate(tree):\n",
    "        sub_trees = tree[split_criterion]\n",
    "\n",
    "        # Print the current node: split criterion\n",
    "        print('|\\t' * depth, end='')\n",
    "        print('+-- [SPLIT: x{0} = {1} {2}]'.format(split_criterion[0], split_criterion[1], split_criterion[2]))\n",
    "\n",
    "        # Print the children\n",
    "        if type(sub_trees) is dict:\n",
    "            pretty_print(sub_trees, depth + 1)\n",
    "        else:\n",
    "            print('|\\t' * (depth + 1), end='')\n",
    "            print('+-- [LABEL = {0}]'.format(sub_trees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_dot_file(dot_string, save_file, image_format='png'):\n",
    "    \"\"\"\n",
    "    Uses GraphViz to render a dot file. The dot file can be generated using\n",
    "        * sklearn.tree.export_graphviz()' for decision trees produced by scikit-learn\n",
    "        * to_graphviz() (function is in this file) for decision trees produced by  your code.\n",
    "    DO NOT MODIFY THIS FUNCTION!\n",
    "    \"\"\"\n",
    "    if type(dot_string).__name__ != 'str':\n",
    "        raise TypeError('visualize() requires a string representation of a decision tree.\\nUse tree.export_graphviz()'\n",
    "                        'for decision trees produced by scikit-learn and to_graphviz() for decision trees produced by'\n",
    "                        'your code.\\n')\n",
    "\n",
    "    # Set path to your GraphViz executable here\n",
    "    os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "    graph = graphviz.Source(dot_string)\n",
    "    graph.format = image_format\n",
    "    graph.render(save_file, view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_graphviz(tree, dot_string='', uid=-1, depth=0):\n",
    "    \"\"\"\n",
    "    Converts a tree to DOT format for use with visualize/GraphViz\n",
    "    DO NOT MODIFY THIS FUNCTION!\n",
    "    \"\"\"\n",
    "\n",
    "    uid += 1       # Running index of node ids across recursion\n",
    "    node_id = uid  # Node id of this node\n",
    "\n",
    "    if depth == 0:\n",
    "        dot_string += 'digraph TREE {\\n'\n",
    "\n",
    "    for split_criterion in tree:\n",
    "        sub_trees = tree[split_criterion]\n",
    "        attribute_index = split_criterion[0]\n",
    "        attribute_value = split_criterion[1]\n",
    "        split_decision = split_criterion[2]\n",
    "\n",
    "        if not split_decision:\n",
    "            # Alphabetically, False comes first\n",
    "            dot_string += '    node{0} [label=\"x{1} = {2}?\"];\\n'.format(node_id, attribute_index, attribute_value)\n",
    "\n",
    "        if type(sub_trees) is dict:\n",
    "            if not split_decision:\n",
    "                dot_string, right_child, uid = to_graphviz(sub_trees, dot_string=dot_string, uid=uid, depth=depth + 1)\n",
    "                dot_string += '    node{0} -> node{1} [label=\"False\"];\\n'.format(node_id, right_child)\n",
    "            else:\n",
    "                dot_string, left_child, uid = to_graphviz(sub_trees, dot_string=dot_string, uid=uid, depth=depth + 1)\n",
    "                dot_string += '    node{0} -> node{1} [label=\"True\"];\\n'.format(node_id, left_child)\n",
    "\n",
    "        else:\n",
    "            uid += 1\n",
    "            dot_string += '    node{0} [label=\"y = {1}\"];\\n'.format(uid, sub_trees)\n",
    "            if not split_decision:\n",
    "                dot_string += '    node{0} -> node{1} [label=\"False\"];\\n'.format(node_id, uid)\n",
    "            else:\n",
    "                dot_string += '    node{0} -> node{1} [label=\"True\"];\\n'.format(node_id, uid)\n",
    "\n",
    "    if depth == 0:\n",
    "        dot_string += '}\\n'\n",
    "        return dot_string\n",
    "    else:\n",
    "        return dot_string, node_id, uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Load the training data\n",
    "    M = np.genfromtxt('./monks-1.train', missing_values=0, skip_header=0, delimiter=',', dtype=int)\n",
    "    ytrn = M[:, 0]\n",
    "    Xtrn = M[:, 1:]\n",
    "\n",
    "    # Load the test data\n",
    "    M = np.genfromtxt('./monks-1.test', missing_values=0, skip_header=0, delimiter=',', dtype=int)\n",
    "    ytst = M[:, 0]\n",
    "    Xtst = M[:, 1:]\n",
    "\n",
    "    # Learn a decision tree of depth 3\n",
    "    decision_tree = id3(Xtrn, ytrn, max_depth=3)\n",
    "\n",
    "    # Pretty print it to console\n",
    "    pretty_print(decision_tree)\n",
    "\n",
    "    # Visualize the tree and save it as a PNG image\n",
    "    dot_str = to_graphviz(decision_tree)\n",
    "    render_dot_file(dot_str, './my_learned_tree')\n",
    "\n",
    "    # Compute the test error\n",
    "    y_pred = [predict_example(x, decision_tree) for x in Xtst]\n",
    "    tst_err = compute_error(ytst, y_pred)\n",
    "\n",
    "    print('Test Error = {0:4.2f}%.'.format(tst_err * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
